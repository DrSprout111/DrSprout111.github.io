---
layout: post
title:  "스케일러의 종류 및 효과 - 1"
date:   2020-09-25 08:43:59
author: Dr. Sprout
categories: Data-Science
tags:	preprocessing scaler study
cover:  "/assets/instacode.png"
---

스케일러를 사용하여 `feature`와 `target`을 정규분포와 비슷하게 만들게 되면 학습이 잘 된다.

무조건 잘 되는건 아니긴 한데, 해 봐서 스코어 올라가면 일단 써본다.

통계학적인 배경지식이 너무 부족해서 일단은 따라하는 시늉만 해본다.

배경지식을 틈틈히 쌓고 있으니 너무 나쁘게 보지는 말아줬으면...

스케일러를 사용하면 데이터 분포를 어떻게 바꿀 수 있는가 확인해보기로 했다.

---

<br>

# 데이터 준비하기

우선 다양한 분포의 데이터를 임의로 만들어 보았다.


```Python
from scipy.stats import skewnorm
import numpy as np

x1 = skewnorm.rvs(0, size=(500,1))
x2 = skewnorm.rvs(-100, size=(500,1))
x3 = skewnorm.rvs(100, size=(500,1))
x4 = np.append(
    np.random.normal(3,4,size=(250,1)),
    np.random.normal(-4,1,size=(250,1)), axis=0)

outlier = np.random.uniform(-20,20,size=(10,1))
x1_out = np.append(x1, outlier, axis=0)
x2_out = np.append(x2, outlier, axis=0)
x3_out = np.append(x3, outlier, axis=0)
x4_out = np.append(x4, outlier, axis=0)
```

만들어진 데이터 분포는 다음과 같다.

```
import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(2,4)
fig.set_size_inches(12,6)
fig.tight_layout()

sns.distplot(x1, bins=50, ax=ax[0, 0])
sns.distplot(x2, bins=50, ax=ax[0, 1])
sns.distplot(x3, bins=50, ax=ax[0, 2])
sns.distplot(x4, bins=50, ax=ax[0, 3])

sns.distplot(x1_out, bins=50, ax=ax[1, 0])
sns.distplot(x2_out, bins=50, ax=ax[1, 1])
sns.distplot(x3_out, bins=50, ax=ax[1, 2])
sns.distplot(x4_out, bins=50, ax=ax[1, 3])

plt.subplots_adjust(wspace=0.3, hspace=0.3)

plt.show()
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_3_0.png)



<br>

# 이상치 없는 정규분포


```
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.preprocessing import QuantileTransformer, PowerTransformer
from sklearn.compose import ColumnTransformer

def show_dist1(x):
  fig, ax = plt.subplots(2,3)
  fig.set_size_inches(9,6)
  fig.tight_layout()

  titles = ['None',
            'MinMax',
            'Standard',
            'Robust',
            'Quantile',
            'Power']
  scalers = [ColumnTransformer([],remainder='passthrough'),
            MinMaxScaler(), 
            StandardScaler(),
            RobustScaler(),
            QuantileTransformer(n_quantiles=50, output_distribution='normal'),
            PowerTransformer()]

  for scaler, title, ax_ in zip(scalers, titles, ax.reshape(-1)):
    res = scaler.fit_transform(x)
    sns.distplot(res, bins=50, ax=ax_)
    ax_.set_title(title)

  plt.subplots_adjust(wspace=0.3, hspace=0.3)
  plt.show()
```


```
show_dist1(x1)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_6_0.png)


전반적으로 다 잘 변환이 되었다.
`QuantileTransformer` 를 사용했을 때 양 쪽으로 꼬리가 생기는게 좀 아쉽다.


<br>

# 이상치 없는 왜정규분포


```
show_dist1(x2)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_9_0.png)



```
show_dist1(x3)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_10_0.png)


`QuantileTransformer` 가 약간 깡패인듯하다. 거의 반 강제로 정규분포화 시키는듯하다.
`PowerTransformer` 도 꽤 괜찮다.
나머지는 왜도를 잡지 못한다.


<br>

## 이상치 없는 쌍곡분포


```
show_dist1(x4)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_13_0.png)


쌍곡분포도 그냥 정규분포화 시킨다. 여기까지 오니까 이놈을 써도 되나 약간 무서워진다.


<br>

## 이상치 있는 정규분포


```
show_dist1(x1_out)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_16_0.png)



<br>

## 이상치 있는 왜정규분포


```
show_dist1(x2_out)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_18_0.png)



```
show_dist1(x3_out)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_19_0.png)



<br>

# 이상치 있는 쌍곡분포


```
show_dist1(x4_out)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_21_0.png)


이상치가 있어도 다른 애들은 정신 못차리는데 `QuantileTransformer` 는 그냥 다 정규분포화 시키는거같다.


<br>

# QuantileTransformer는 뭐하는 놈이지?

뭐하는 놈인가 싶어서 대충 등간격 배열을 때려넣어 봤다.


```
def show_scaler_function(x):
  fig, ax = plt.subplots(2,3)
  fig.set_size_inches(9,6)
  fig.tight_layout()

  titles = ['None',
            'MinMax',
            'Standard',
            'Robust',
            'Quantile',
            'Power']
  scalers = [ColumnTransformer([],remainder='passthrough'),
            MinMaxScaler(), 
            StandardScaler(),
            RobustScaler(),
            QuantileTransformer(n_quantiles=50, output_distribution='normal'),
            PowerTransformer()]

  a = np.linspace(-30,30, 100).reshape(-1,1)

  for scaler, title, ax_ in zip(scalers, titles, ax.reshape(-1)):
    scaler.fit(x)
    res = scaler.transform(a)
    ax_.plot(a, res)
    ax_.set_title(title)
    ax_.set_xlabel('Before')
    ax_.set_ylabel('After')

  plt.subplots_adjust(wspace=0.5, hspace=0.3)
  plt.show()
```


```
show_scaler_function(x4_out)
```


![png]({{site.url}}/assets/2020-09-25-Post2_files/2020-09-25-Post2_25_0.png)


`QuantileTransformer` 는 `n_quantile`개의 분위수를 찾아내서 분위수에 맞는 위치에다 박아버리는 놈이다.

그래서 `n_quantile` 수가 `fit` 하는 데이터 수보다 많으면 에러가 뜬다. 학생 100 명 키 순으로 세워 놓고 3.14번째로 키 큰 학생을 부를 순 없으니까.



<br>

# 잡다한 스킬


<br>

## 스케일러 붙이기

어차피 `feature`를 스케일러에 넣고 바로 회귀모델에 넣을 거, 그냥 둘이 묶어버릴 수 있다.

묶으면 좋다고 하더라. 어디에 좋은지는 지금은 기억이 안난다...

```
from sklearn.pipeline import Pipline
from sklearn.ensemble import RandomForestRegressor

model = Pipeline([('스케일러임', RobustScaler()),
                  ('아무나무', RandomForestRegressor())])

model.fit(x_train, y_train)        # 훈련은 이렇게

y_test_hat = model.predict(x_test) # 예측은 저렇게

model.score(x_test,y_test)         # 평가는 그렇게

```

`target`도 스케일러나 트랜스포머에 넣어서 정규분포화시킨 후 사용하면 스코어가 꽤 좋아지기도 한다.

주의 할 점은 모델에서 나온 `output`을 `inverse_transform` 시켜 준 후 스코어를 구해야 한다는 것이다.

왜 그런지는 스코어 계산이 어떻게 되는지만 알면 당연히 알 수 있다.

이것도 회귀모델 앞단에 끼워넣을 수 있다.

```
from sklearn.compose import TransformedTargetRegressor

# transfomer를 넣을 수도 있고,

model = TransformedTargetRegressor(regressor=RandomForestRegressor(), 
                                   transformer=RobustScaler())
```

변환 함수를 정의해서 넣을 수도 있다 (둘 다 동시에 넣는 것은 안된다).

```
def target_trans(y):
    return np.log1p(y)

def target_inv_trans(y):
    return np.expm1(y)

model = TransformedTargetRegressor(regressor=RandomForestRegressor(),
                                   func=target_trans, 
                                   inverse_func=target_inv_trans)

```


쓸 일이 오게 될지는 몰라도 `check_inverse`를 `False`로 놓으면 `output`을 `inverse_transform`을 안 거치고 나온다.

용도가 나름 있겠지..

```

model = TransformedTargetRegressor(regressor=RandomForestRegressor(),
                                   func=None, 
                                   inverse_func=None, 
                                   check_inverse=False)

```




<br>

## 2단 스케일러

스케일러 하나로는 좀 모자르다 싶으면 이렇게 추가해도 에러는 안 난다.

에러는 안 나도 쓸모가 있을지는 장담 못한다.


```
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

pipe = Pipeline([('quantile', QuantileTransformer(n_quantiles=50, output_distribution='normal')),
                 ('function', FunctionTransformer(np.arcsinh))
                 ])
```


<br>

## Column마다 다른 스케일러 적용하기

Feature마다 잘 맞는 스케일러가 다를 수 있다. 그럴때는 `ColumnTransformer` 를 쓰면 된다. 

이때, `remainder`를 `passthrough`로 놓지 않으면 스케일러를 안 쓴 행들이 반환되지 않는다.


```


col_trans = ColumnTransformer(
    [('norm1', MinMaxScaler(), [0,1,2,3,4]),
     ('norm2', QuantileTransformer(n_quantiles=50, output_distribution='normal'), [5,6,7,8,9])],
     remainder='passthrough'
     )

model = Pipeline([('스케일러임', col_trans),
                  ('아무나무', RandomForestRegressor())])
```

<br>

## 끔찍한 혼종

전후방 쌍 스케일러

```

col_trans = ColumnTransformer(
    [('norm1', MinMaxScaler(), [0,1,2,3,4]),
     ('norm2', QuantileTransformer(n_quantiles=50, output_distribution='normal'), [5,6,7,8,9])],
     remainder='passthrough'
     )

model = Pipeline([('스케일러임', col_trans),
                  ('아무나무', RandomForestRegressor())])

model = TransformedTargetRegressor(regressor=model, 
                                   transformer=RobustScaler())

```

<br>

# 마무리

어떤 스케일러를 사용할지는 데이터셋에 따라 다 달라서 경험이 쌓이기 전에는 결국 다 해봐야 할 것 같다. 
